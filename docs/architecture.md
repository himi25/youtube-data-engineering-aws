Architecture â€“ YouTube Data Engineering Pipeline (AWS)

1. Overview

-> This project implements an end-to-end, serverless data engineering pipeline on AWS to process YouTube trending video data
-> The system ingests raw CSV and JSON data from Kaggle into Amazon S3
-> The data is transformed into an optimized columnar Parquet format using AWS Lambda
-> The processed data is cataloged using AWS Glue and queried using Amazon Athena
-> The architecture is designed to be scalable, cost-efficient, and fully serverless

2. High-Level Architecture
Data Flow

Kaggle Dataset
-> Amazon S3 (Raw Data Bucket)
-> AWS Lambda (ETL: Clean and Convert to Parquet)
-> Amazon S3 (Cleansed Data Bucket)
-> AWS Glue Data Catalog + Crawler
-> Amazon Athena (SQL Analytics)

3. Components and Their Roles
3.1 Amazon S3

-> Amazon S3 is used as the data lake storage layer
-> Two logical storage layers are implemented:
-> Raw data bucket stores original YouTube CSV and JSON files uploaded from Kaggle
-> Cleansed data bucket stores transformed and cleaned data in Parquet format generated by the ETL process
-> Raw data is organized using a Hive-style partitioned folder structure by region
-> Partitioning by region improves query performance and reduces Amazon Athena scan costs

3.2 AWS Lambda (ETL Layer)

-> AWS Lambda is used as a serverless ETL processing layer implemented using Python
-> The Lambda function performs the following tasks:
-> Reads raw CSV and JSON files from the raw S3 bucket
-> Normalizes and flattens nested JSON structures using pandas
-> Cleans and standardizes the dataset
-> Converts the data into Parquet format using awswrangler
-> Writes the processed data to the cleansed S3 bucket
-> Lambda was chosen because it requires no server management, scales automatically, and is cost-efficient for batch and event-driven workloads

3.3 AWS Glue Data Catalog

-> AWS Glue is used to manage metadata for the processed dataset
-> A Glue database is created for the cleansed data
-> A Glue Crawler scans the cleansed S3 bucket containing Parquet files
-> The crawler automatically discovers schema and creates tables in the Glue Data Catalog
-> These tables are used by Amazon Athena for SQL querying
-> An initial attempt to crawl the raw JSON data failed due to multi-line and nested JSON format issues
-> This led to introducing the Lambda-based ETL step to convert the data into Parquet before cataloging

3.4 Amazon Athena

-> Amazon Athena is used as the serverless SQL query engine for analytics
-> Queries run directly on data stored in Amazon S3 using the Glue Data Catalog
-> Used for exploratory data analysis and reporting
-> Typical queries include aggregations by region, category, views, likes, and other metrics
-> Query performance is improved due to the use of Parquet format and partitioned data layout

3.5 IAM (Security)

-> AWS Identity and Access Management (IAM) is used to control access to all services
-> IAM roles are created to allow Lambda to read and write data in S3 and interact with AWS Glue
-> A Glue service role is used to allow crawlers to access S3 buckets
-> Least-privilege access is enforced, and the root account is not used for daily development work

4. Design Decisions

-> Parquet format is used for columnar storage, better compression, and faster Athena queries
-> A serverless architecture using Lambda, Glue, and Athena avoids infrastructure management and scales automatically
-> A partitioned data layout by region reduces query scan size and cost
-> Glue Crawlers are used to automate schema discovery instead of manual table creation
-> Separate raw and cleansed S3 buckets are used to clearly separate ingestion and processed data layers

5. Scalability and Reliability

-> The pipeline is fully serverless and scales automatically with data volume
-> New data can be added to the raw S3 bucket and reprocessed using the same Lambda ETL function
-> Glue Crawlers can be scheduled to keep the catalog updated as new data arrives
-> Athena can handle large-scale analytical queries without any infrastructure provisioning

6. Future Improvements

-> Add orchestration using AWS Step Functions or Amazon EventBridge
-> Automate data ingestion using scheduled jobs or API-based ingestion
-> Add data quality checks and validation steps in the ETL pipeline
-> Build dashboards using Amazon QuickSight or other BI tools
-> Implement incremental processing instead of full reprocessing

7. Architecture Diagram

-> The following diagram represents the high-level architecture of the pipeline:

![alt text](image.png)